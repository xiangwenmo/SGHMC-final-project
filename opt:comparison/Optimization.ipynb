{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for Optimization\n",
    "\n",
    "\n",
    "\n",
    "Define usual functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_batch(data, batch_size, seed = 123):\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if n % batch_size !=0:\n",
    "        print('%d data dropped during batching' % (n%batch_size))\n",
    "    sample_size = (n // batch_size)*batch_size\n",
    "        \n",
    "    #shuffle\n",
    "    np.random.seed(seed)\n",
    "    idx = np.arange(n)\n",
    "    np.random.shuffle(idx)\n",
    "    n_batch = n//batch_size\n",
    "    data = data[idx]\n",
    "    data = data[:sample_size].reshape(batch_size, p, n_batch)\n",
    "    return(data, n_batch)\n",
    "\n",
    "def is_pos_def(A):\n",
    "    '''function to check if matrix is positive definite'''\n",
    "    return np.all(np.linalg.eigvals(A) > 0)\n",
    "\n",
    "def sghmc(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    p = theta_0.shape[0]\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    theta_samp = np.zeros((p, epochs))\n",
    "    theta_samp[:,0] = theta_0\n",
    "    \n",
    "    B_hat = 0.5*eps*V_hat\n",
    "    \n",
    "    if not is_pos_def(2*(C-B_hat)*eps):\n",
    "        print(\"error: noise term is not positive definite\")\n",
    "        return\n",
    "        \n",
    "    sqrt_noise = np.sqrt(2*(C-B_hat)*eps)\n",
    "    \n",
    "    sqrtM = np.sqrt(la.inv(Minv))\n",
    "    r = np.random.multivariate_normal(np.zeros(p), sqrtM).reshape(p, -1)\n",
    "    \n",
    "    dat_batch, nbatches = data_batch(data, batch_size)\n",
    "    for i in range(epochs-1):\n",
    "        \n",
    "        theta = theta_samp[:,i]\n",
    "        r = np.random.multivariate_normal(np.zeros(p), sqrtM).reshape(p, -1)\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            theta = theta + (eps*Minv@r).ravel()\n",
    "            gradU_batch = gradU(theta, dat_batch[:,:, batch], n, batch_size).reshape(p, -1)\n",
    "            r = r-eps*gradU_batch - eps*C@Minv@r \\\n",
    "                + np.random.multivariate_normal(np.zeros(p), sqrt_noise).reshape(p, -1)\n",
    "            \n",
    "        theta_samp[:,i+1] = theta\n",
    "            \n",
    "    return theta_samp[:, burns:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean algorithm: use cholesky decomposition, and use cholesky based sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc_clean(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
    "    \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    p = theta_0.shape[0]\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    theta_samp = np.zeros((p, epochs))\n",
    "    theta_samp[:,0] = theta_0\n",
    "    \n",
    "    B_hat = 0.5*eps*V_hat\n",
    "    \n",
    "    if not is_pos_def(2*(C-B_hat)*eps):\n",
    "        print(\"error: noise term is not positive definite\")\n",
    "        return\n",
    "    \n",
    "    sqrt_noise = np.linalg.cholesky(2*(C-B_hat)*eps)\n",
    "    \n",
    "    sqrtM = np.linalg.cholesky(np.linalg.inv(Minv))\n",
    "    r = sqrtM@np.random.normal(size = p).reshape(p, -1)\n",
    "    \n",
    "    dat_batch, nbatches = data_batch(data, batch_size)\n",
    "    for i in range(epochs-1):\n",
    "        \n",
    "        theta = theta_samp[:,i]\n",
    "        r = sqrtM@np.random.normal(size = p).reshape(p, -1)\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            theta = theta + (eps*Minv@r).ravel()\n",
    "            gradU_batch = gradU(theta, dat_batch[:,:, batch], n, batch_size).reshape(p, -1)\n",
    "            r = r-eps*gradU_batch - eps*C@Minv@r + sqrt_noise@np.random.normal(size = p).reshape(p, -1)\n",
    "            \n",
    "        theta_samp[:,i+1] = theta\n",
    "            \n",
    "    return theta_samp[:, burns:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, float64, int64\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import seaborn as sns\n",
    "from autograd import jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### approach 1: JIT to dat_batch and sghmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def dat_batch_numba(data, batch_size, seed = 123):\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if n % batch_size !=0:\n",
    "        print('%d data dropped during batching' % (n%batch_size))\n",
    "    sample_size = (n // batch_size)*batch_size\n",
    "        \n",
    "    #shuffle\n",
    "    np.random.seed(seed)\n",
    "    idx = np.arange(n)\n",
    "    np.random.shuffle(idx)\n",
    "    n_batch = n//batch_size\n",
    "    data = data[idx]\n",
    "    data = data[:sample_size].reshape(batch_size, p, n_batch)\n",
    "    return(data, n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-cb78ff7807cb>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"sghmc_numba\" failed type inference due to: Untyped global name 'is_pos_def': cannot determine Numba type of <class 'function'>\n",
      "\n",
      "File \"<ipython-input-6-cb78ff7807cb>\", line 14:\n",
      "def sghmc_numba(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    \n",
      "    if not is_pos_def(2*(C-B_hat)*eps):\n",
      "    ^\n",
      "\n",
      "  @jit([float64[:,:](float64[:], float64, float64[:,:], float64[:,:], float64[:], float64[:,:], int64, int64, float64[:,:], int64, int64)], cache = True)\n",
      "<ipython-input-6-cb78ff7807cb>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"sghmc_numba\" failed type inference due to: Untyped global name 'is_pos_def': cannot determine Numba type of <class 'function'>\n",
      "\n",
      "File \"<ipython-input-6-cb78ff7807cb>\", line 14:\n",
      "def sghmc_numba(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    \n",
      "    if not is_pos_def(2*(C-B_hat)*eps):\n",
      "    ^\n",
      "\n",
      "  @jit([float64[:,:](float64[:], float64, float64[:,:], float64[:,:], float64[:], float64[:,:], int64, int64, float64[:,:], int64, int64)], cache = True)\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"sghmc_numba\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\n",
      "File \"<ipython-input-6-cb78ff7807cb>\", line 4:\n",
      "def sghmc_numba(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "        \n",
      "    np.random.seed(seed)\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-6-cb78ff7807cb>\", line 4:\n",
      "def sghmc_numba(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "        \n",
      "    np.random.seed(seed)\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-6-cb78ff7807cb>:1: NumbaWarning: Cannot cache compiled function \"sghmc_numba\" as it uses lifted loops\n",
      "  @jit([float64[:,:](float64[:], float64, float64[:,:], float64[:,:], float64[:], float64[:,:], int64, int64, float64[:,:], int64, int64)], cache = True)\n"
     ]
    }
   ],
   "source": [
    "@jit([float64[:,:](float64[:], float64, float64[:,:], float64[:,:], float64[:], float64[:,:], int64, int64, float64[:,:], int64, int64)], cache = True)\n",
    "def sghmc_numba(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
    "        \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    p = theta_0.shape[0]\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    theta_samp = np.zeros((p, epochs))\n",
    "    theta_samp[:,0] = theta_0\n",
    "    \n",
    "    B_hat = 0.5*eps*V_hat\n",
    "    \n",
    "    if not is_pos_def(2*(C-B_hat)*eps):\n",
    "        print(\"error: noise term is not positive definite\")\n",
    "        return\n",
    "    \n",
    "    sqrt_noise = np.linalg.cholesky(2*(C-B_hat)*eps)\n",
    "    \n",
    "    sqrtM = np.linalg.cholesky(np.linalg.inv(Minv))\n",
    "    r = sqrtM@np.random.normal(size = p).reshape(p, -1)\n",
    "    \n",
    "    dat_batch, nbatches = dat_batch_numba(data, batch_size)\n",
    "    for i in range(epochs-1):\n",
    "        \n",
    "        theta = theta_samp[:,i]\n",
    "        r = sqrtM@np.random.normal(size = p).reshape(p, -1)\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            theta = theta + (eps*Minv@r).ravel()\n",
    "            gradU_batch = gradU(theta, dat_batch[:,:, batch], n, batch_size).reshape(p, -1)\n",
    "            r = r-eps*gradU_batch - eps*C@Minv@r + sqrt_noise@np.random.normal(size = p).reshape(p, -1)\n",
    "            \n",
    "        theta_samp[:,i+1] = theta\n",
    "            \n",
    "    return theta_samp[:, burns:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### approach 2: add JIT only to sghmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-5b232f9a54e4>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"sghmc_numba2\" failed type inference due to: Untyped global name 'is_pos_def': cannot determine Numba type of <class 'function'>\n",
      "\n",
      "File \"<ipython-input-7-5b232f9a54e4>\", line 14:\n",
      "def sghmc_numba2(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    \n",
      "    if not is_pos_def(2*(C-B_hat)*eps):\n",
      "    ^\n",
      "\n",
      "  @jit([float64[:,:](float64[:], float64, float64[:,:], float64[:,:], float64[:], float64[:,:], int64, int64, float64[:,:], int64, int64)], cache = True)\n",
      "<ipython-input-7-5b232f9a54e4>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"sghmc_numba2\" failed type inference due to: Untyped global name 'is_pos_def': cannot determine Numba type of <class 'function'>\n",
      "\n",
      "File \"<ipython-input-7-5b232f9a54e4>\", line 14:\n",
      "def sghmc_numba2(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    \n",
      "    if not is_pos_def(2*(C-B_hat)*eps):\n",
      "    ^\n",
      "\n",
      "  @jit([float64[:,:](float64[:], float64, float64[:,:], float64[:,:], float64[:], float64[:,:], int64, int64, float64[:,:], int64, int64)], cache = True)\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"sghmc_numba2\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\n",
      "File \"<ipython-input-7-5b232f9a54e4>\", line 4:\n",
      "def sghmc_numba2(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    \n",
      "    np.random.seed(seed)\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-7-5b232f9a54e4>\", line 4:\n",
      "def sghmc_numba2(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    \n",
      "    np.random.seed(seed)\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-7-5b232f9a54e4>:1: NumbaWarning: Cannot cache compiled function \"sghmc_numba2\" as it uses lifted loops\n",
      "  @jit([float64[:,:](float64[:], float64, float64[:,:], float64[:,:], float64[:], float64[:,:], int64, int64, float64[:,:], int64, int64)], cache = True)\n"
     ]
    }
   ],
   "source": [
    "@jit([float64[:,:](float64[:], float64, float64[:,:], float64[:,:], float64[:], float64[:,:], int64, int64, float64[:,:], int64, int64)], cache = True)\n",
    "def sghmc_numba2(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    p = theta_0.shape[0]\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    theta_samp = np.zeros((p, epochs))\n",
    "    theta_samp[:,0] = theta_0\n",
    "    \n",
    "    B_hat = 0.5*eps*V_hat\n",
    "    \n",
    "    if not is_pos_def(2*(C-B_hat)*eps):\n",
    "        print(\"error: noise term is not positive definite\")\n",
    "        return\n",
    "    \n",
    "    sqrt_noise = np.linalg.cholesky(2*(C-B_hat)*eps)\n",
    "    \n",
    "    sqrtM = np.linalg.cholesky(np.linalg.inv(Minv))\n",
    "    r = sqrtM@np.random.normal(size = p).reshape(p, -1)\n",
    "    \n",
    "    dat_batch, nbatches = data_batch(data, batch_size)\n",
    "    for i in range(epochs-1):\n",
    "        \n",
    "        theta = theta_samp[:,i]\n",
    "        r = sqrtM@np.random.normal(size = p).reshape(p, -1)\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            theta = theta + (eps*Minv@r).ravel()\n",
    "            gradU_batch = gradU(theta, dat_batch[:,:, batch], n, batch_size).reshape(p, -1)\n",
    "            r = r-eps*gradU_batch - eps*C@Minv@r +sqrt_noise@np.random.normal(size = p).reshape(p, -1)\n",
    "            \n",
    "        theta_samp[:,i+1] = theta\n",
    "            \n",
    "    return theta_samp[:, burns:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Mixture normals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.array([-3,3]).reshape(2,1)\n",
    "\n",
    "def lprior(theta):\n",
    "    return (-1/(2*10))*theta.T@theta\n",
    "\n",
    "def ldatap(theta, x):\n",
    "    return np.log(0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2))\n",
    "\n",
    "def U(theta, x, n, batch_size):\n",
    "    return -lprior(theta) - (n/batch_size)*sum(ldatap(theta, x))\n",
    "\n",
    "gradU = jacobian(U, argnum = 0)\n",
    "\n",
    "@jit\n",
    "def lprior_numba(theta):\n",
    "    return (-1/(2*10))*theta.T@theta\n",
    "\n",
    "@jit\n",
    "def ldatap_numba(theta, x):\n",
    "    return np.log(0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2))\n",
    "\n",
    "@jit\n",
    "def U_numba(theta, x, n, batch_size):\n",
    "    return -lprior_numba(theta) - (n/batch_size)*sum(ldatap_numba(theta, x))\n",
    "\n",
    "gradU_numba = jacobian(U_numba, argnum = 0)\n",
    "\n",
    "#Set up data and parameters\n",
    "np.random.seed(123)\n",
    "n = 100\n",
    "x = np.r_[\n",
    "    np.random.normal(mu[0], 1, n),\n",
    "    np.random.normal(mu[1], 1, n)].reshape(-1,1)\n",
    "\n",
    "theta_0 = np.array([-3, 3]) #start at true value\n",
    "eps = 0.01\n",
    "V_hat = np.eye(2)\n",
    "C = np.eye(2)\n",
    "epochs = 500\n",
    "burns = 200\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5 s ± 51.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "4.28 s ± 46.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc(gradU, eps, C, np.eye(2), theta_0, V_hat, epochs, burns, x, batch_size)\n",
    "%timeit sghmc_clean(gradU, eps, C, np.eye(2), theta_0, V_hat, epochs, burns, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-93-6d01c0162750>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"dat_batch_numba\" failed type inference due to: Invalid use of Function(<built-in function mod>) with argument(s) of type(s): (Literal[str](%d data dropped during batching), int64)\n",
      "Known signatures:\n",
      " * (int64, int64) -> int64\n",
      " * (int64, uint64) -> int64\n",
      " * (uint64, int64) -> int64\n",
      " * (uint64, uint64) -> uint64\n",
      " * (float32, float32) -> float32\n",
      " * (float64, float64) -> float64\n",
      " * parameterized\n",
      "In definition 0:\n",
      "    All templates rejected with literals.\n",
      "In definition 1:\n",
      "    All templates rejected without literals.\n",
      "In definition 2:\n",
      "    All templates rejected with literals.\n",
      "In definition 3:\n",
      "    All templates rejected without literals.\n",
      "This error is usually caused by passing an argument of a type that is unsupported by the named function.\n",
      "[1] During: typing of intrinsic-call at <ipython-input-93-6d01c0162750> (6)\n",
      "\n",
      "File \"<ipython-input-93-6d01c0162750>\", line 6:\n",
      "def dat_batch_numba(data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    if n % batch_size !=0:\n",
      "        print('%d data dropped during batching' % (n%batch_size))\n",
      "        ^\n",
      "\n",
      "  @jit\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"dat_batch_numba\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"<ipython-input-93-6d01c0162750>\", line 2:\n",
      "@jit\n",
      "def dat_batch_numba(data, batch_size, seed = 123):\n",
      "^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-93-6d01c0162750>\", line 2:\n",
      "@jit\n",
      "def dat_batch_numba(data, batch_size, seed = 123):\n",
      "^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-100-cb78ff7807cb>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"sghmc_numba\" failed type inference due to: non-precise type pyobject\n",
      "[1] During: typing of argument at <ipython-input-100-cb78ff7807cb> (24)\n",
      "\n",
      "File \"<ipython-input-100-cb78ff7807cb>\", line 24:\n",
      "def sghmc_numba(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    dat_batch, nbatches = dat_batch_numba(data, batch_size)\n",
      "    for i in range(epochs-1):\n",
      "    ^\n",
      "\n",
      "  @jit([float64[:,:](float64[:], float64, float64[:,:], float64[:,:], float64[:], float64[:,:], int64, int64, float64[:,:], int64, int64)], cache = True)\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"sghmc_numba\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"<ipython-input-100-cb78ff7807cb>\", line 24:\n",
      "def sghmc_numba(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    dat_batch, nbatches = dat_batch_numba(data, batch_size)\n",
      "    for i in range(epochs-1):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-100-cb78ff7807cb>\", line 24:\n",
      "def sghmc_numba(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    dat_batch, nbatches = dat_batch_numba(data, batch_size)\n",
      "    for i in range(epochs-1):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.34 s ± 52.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_numba(gradU, eps, C, np.eye(2), theta_0, V_hat, epochs, burns, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-101-5b232f9a54e4>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"sghmc_numba2\" failed type inference due to: non-precise type pyobject\n",
      "[1] During: typing of argument at <ipython-input-101-5b232f9a54e4> (24)\n",
      "\n",
      "File \"<ipython-input-101-5b232f9a54e4>\", line 24:\n",
      "def sghmc_numba2(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    dat_batch, nbatches = data_batch(data, batch_size)\n",
      "    for i in range(epochs-1):\n",
      "    ^\n",
      "\n",
      "  @jit([float64[:,:](float64[:], float64, float64[:,:], float64[:,:], float64[:], float64[:,:], int64, int64, float64[:,:], int64, int64)], cache = True)\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"sghmc_numba2\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"<ipython-input-101-5b232f9a54e4>\", line 24:\n",
      "def sghmc_numba2(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    dat_batch, nbatches = data_batch(data, batch_size)\n",
      "    for i in range(epochs-1):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-101-5b232f9a54e4>\", line 24:\n",
      "def sghmc_numba2(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    dat_batch, nbatches = data_batch(data, batch_size)\n",
      "    for i in range(epochs-1):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.34 s ± 66.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_numba2(gradU, eps, C, np.eye(2), theta_0, V_hat, epochs, burns, x, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradU(theta, data, n, batch_size):\n",
    "    return (-n*np.sum(data-theta, axis = 0)/batch_size - np.sum(theta))\n",
    "\n",
    "@jit\n",
    "def gradU_numba(theta, data, n, batch_size):\n",
    "    return (-n*np.sum(data-theta, axis = 0)/batch_size - np.sum(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 100\n",
    "x = np.random.normal(size = (10000, p))\n",
    "V = np.eye(p)\n",
    "eps = 0.01\n",
    "theta_0 = np.zeros(p)\n",
    "C = np.eye(p)\n",
    "burn = 100\n",
    "epochs = 200\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.09 s ± 331 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc(gradU, eps, C, np.eye(p), theta_0, V, epochs, burn, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.36 s ± 84.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_clean(gradU_numba, eps, C, np.eye(p), theta_0, V, epochs, burn, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.25 s ± 94 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_clean(gradU, eps, C, np.eye(p), theta_0, V, epochs, burn, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.45 s ± 85.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_numba(gradU, eps, C, np.eye(p), theta_0, V, epochs, burn, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-5b232f9a54e4>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"sghmc_numba2\" failed type inference due to: non-precise type pyobject\n",
      "[1] During: typing of argument at <ipython-input-7-5b232f9a54e4> (24)\n",
      "\n",
      "File \"<ipython-input-7-5b232f9a54e4>\", line 24:\n",
      "def sghmc_numba2(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    dat_batch, nbatches = data_batch(data, batch_size)\n",
      "    for i in range(epochs-1):\n",
      "    ^\n",
      "\n",
      "  @jit([float64[:,:](float64[:], float64, float64[:,:], float64[:,:], float64[:], float64[:,:], int64, int64, float64[:,:], int64, int64)], cache = True)\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"sghmc_numba2\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"<ipython-input-7-5b232f9a54e4>\", line 24:\n",
      "def sghmc_numba2(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    dat_batch, nbatches = data_batch(data, batch_size)\n",
      "    for i in range(epochs-1):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-7-5b232f9a54e4>\", line 24:\n",
      "def sghmc_numba2(gradU, eps, C, Minv, theta_0, V_hat, epochs, burns, data, batch_size, seed = 123):\n",
      "    <source elided>\n",
      "    dat_batch, nbatches = data_batch(data, batch_size)\n",
      "    for i in range(epochs-1):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16 s ± 71.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_numba2(gradU, eps, C, np.eye(p), theta_0, V, epochs, burn, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 10\n",
    "x = np.random.normal(size = (10000, p))\n",
    "V = np.eye(p)\n",
    "eps = 0.01\n",
    "theta_0 = np.zeros(p)\n",
    "C = np.eye(p)\n",
    "burn = 100\n",
    "epochs = 200\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22 s ± 13.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "440 ms ± 13.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "462 ms ± 15.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "457 ms ± 16.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc(gradU, eps, C, np.eye(p), theta_0, V, epochs, burn, x, batch_size)\n",
    "%timeit sghmc_clean(gradU, eps, C, np.eye(p), theta_0, V, epochs, burn, x, batch_size)\n",
    "%timeit sghmc_numba(gradU, eps, C, np.eye(p), theta_0, V, epochs, burn, x, batch_size)\n",
    "%timeit sghmc_numba2(gradU, eps, C, np.eye(p), theta_0, V, epochs, burn, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
